{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etap 2\n",
    "Z ostatniego etapu otrzymaliśmy zdjęcia przygotowane do następnego etapu i zapisane w foldderze 'output'.\n",
    "Budując i testując modele musimy zwrócić uwagę na brak zbalansowania zbioru. Będziemy się starać balansować zbiór, tak aby Accuracy wykorzystywane do stopnia wytrenowania modelu było adekwatną miarą oceny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703/2\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.fromarray(self.images[idx])  \n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long) \n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funckcje do treningu i ewaluacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, test_loader, epochs=10, learning_rate=0.001, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    print(f\"Using device: {device}\") \n",
    "    model.to(device)\n",
    "  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    print('PRZED EPOKAMI')\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        model.train()\n",
    "        print('po model.train')\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        print('po zerowaniu poprawnych i całkowitych')\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_acc = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_acc:.2f}%\")\n",
    "    print('po epokach')\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_more_metrics(model, train_loader, test_loader, epochs=10, learning_rate=0.001, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    print(f\"Using device: {device}\") \n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())  \n",
    "            all_predictions.extend(predicted.cpu().numpy())  \n",
    "        \n",
    "        train_acc = 100 * correct / total\n",
    "        train_precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "        train_recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "        train_f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1: {train_f1:.4f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    test_acc = 100 * correct / total\n",
    "    test_precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    test_recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    test_f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    test_conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", test_conf_matrix)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_list(output_root):\n",
    "    X = []\n",
    "    y = []\n",
    "    label_to_index = {\n",
    "        label: idx for idx, label in enumerate({'AVM', 'Normal', 'Ulcer'})\n",
    "    }\n",
    "    print(label_to_index)\n",
    "    for root, _, files in os.walk(output_root):\n",
    "        for file in files:\n",
    "            if file.endswith(\".bmp\"): \n",
    "                input_path = os.path.join(root, file)\n",
    "\n",
    "                base, ext = os.path.splitext(file)\n",
    "                new_filename = f\"{base}{ext}\"\n",
    "                image = cv2.imread(input_path)\n",
    "\n",
    "                X.append(image)\n",
    "                y.append(label_to_index[new_filename.split(\"_\")[0]])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pierwsze podejście\n",
    "Do tej grupy modelów balansowanie zbiorów uzyskaliśmy za pomocą obrotów i odbić i ponownego zapisu plików do folderu 'output_balanced'. Ze względu na konieczność przechowywania dużej liczby zdjęć nie jest to najlepsze wyjście, ale jest to nasz punkt startowy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balansowanie - Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja balansująca klasy. Wynik folder 'output_balanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_root = \"output\"\n",
    "output_root = \"output_balanced\"\n",
    "\n",
    "augmentation_times = {\n",
    "    \"Normal\": [\"h\", \"v\"],  # 2 razy więcej \n",
    "    \"AVM\": [\"h90\", \"h180\", \"h270\", \"v90\", \"v180\", \"v270\"],  # 6 razy więcej\n",
    "    \"Ulcer\": [\"h\", \"h90\", \"h180\", \"h270\", \"v\", \"v90\", \"v180\", \"v270\"],  # 8 razy więcej \n",
    "}\n",
    "\n",
    "for root, _, files in os.walk(input_root):\n",
    "    class_name = os.path.basename(root)  \n",
    "\n",
    "    if class_name not in augmentation_times:\n",
    "        continue \n",
    "\n",
    "    for file in files:\n",
    "        if not file.endswith(\".bmp\"):\n",
    "            continue \n",
    "\n",
    "        input_path = os.path.join(root, file)\n",
    "\n",
    "        relative_path = os.path.relpath(root, input_root)\n",
    "        output_dir = os.path.join(output_root, relative_path)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        base, ext = os.path.splitext(file)\n",
    "        variants = augmentation_times[class_name]\n",
    "\n",
    "        image = cv2.imread(input_path)\n",
    "\n",
    "        transformed_images = {\n",
    "            \"base\" : image,\n",
    "            \"h\": cv2.flip(image, 1),\n",
    "            \"v\": cv2.flip(image, 0),\n",
    "            \"90\": cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE),\n",
    "            \"180\": cv2.rotate(image, cv2.ROTATE_180),\n",
    "            \"270\": cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE),\n",
    "            \"h90\": cv2.flip(cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE), 1),\n",
    "            \"h180\": cv2.flip(cv2.rotate(image, cv2.ROTATE_180), 1),\n",
    "            \"h270\": cv2.flip(cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE), 1),\n",
    "            \"v90\": cv2.flip(cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE), 0),\n",
    "            \"v180\": cv2.flip(cv2.rotate(image, cv2.ROTATE_180), 0),\n",
    "            \"v270\": cv2.flip(cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE), 0)\n",
    "        }\n",
    "\n",
    "        # Zapisujemy tylko te transformacje, które są w augmentation_config dla danej klasy\n",
    "        for variant in variants:\n",
    "            output_path = os.path.join(output_dir, f\"{base}{variant}{ext}\")\n",
    "            cv2.imwrite(output_path, transformed_images[variant])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdzenie rozkładu klas po balansowaniu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nazwa klasy</th>\n",
       "      <th>Liczba zdjęć</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVM</td>\n",
       "      <td>4008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Normal</td>\n",
       "      <td>4304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ulcer</td>\n",
       "      <td>3720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Nazwa klasy  Liczba zdjęć\n",
       "0         AVM          4008\n",
       "1      Normal          4304\n",
       "2       Ulcer          3720"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_folder = \"output_balanced\"\n",
    "output_data = []  \n",
    "\n",
    "for folder in os.listdir(parent_folder):\n",
    "    folder_path = os.path.join(parent_folder, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        count = sum(1 for f in os.listdir(folder_path) if f.endswith(\".bmp\"))\n",
    "        output_data.append([folder, count])  \n",
    "df_output = pd.DataFrame(output_data, columns=[\"Nazwa klasy\", \"Liczba zdjęć\"])\n",
    "df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procent zbalansowania: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procent zbalansowania: 86.43%\n"
     ]
    }
   ],
   "source": [
    "proc_balanced = df_output.loc[2, \"Liczba zdjęć\"] /df_output.loc[1, \"Liczba zdjęć\"] *100\n",
    "print(f\"Procent zbalansowania: {proc_balanced:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podział dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratify w celu zachowania równowagi pomiędzy zbiorami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ulcer': 0, 'AVM': 1, 'Normal': 2}\n"
     ]
    }
   ],
   "source": [
    "X, y = save_to_list(\"output_balanced\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123, stratify=y)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=123, stratify=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla zbioru treningowego dodajemy rotacje odpicia w celu większego urozmaicenia zdjęć (augumentacja). Dla zbioru testowego tylko zmiana rozmiaru i normalizacja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),          \n",
    "    transforms.RandomRotation((-30,30)),  \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),                  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),                  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])  \n",
    "\n",
    "\n",
    "train_dataset = ImageDataset(X_train, y_train, transform=train_transform)\n",
    "test_dataset = ImageDataset(X_test, y_test, transform=test_transform)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Droput(0.5) <- zapobiega overfittingowi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 28 * 28, 512),  # ewentualnie dodać jeszcze jedną warstwę pośrednią\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5), # zapobieganie overfittingowi\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/15, Loss: 0.8582, Train Acc: 60.57%, Precision: 0.6136, Recall: 0.6057, F1: 0.6051\n",
      "Epoch 2/15, Loss: 0.5900, Train Acc: 76.80%, Precision: 0.7727, Recall: 0.7680, F1: 0.7687\n",
      "Epoch 3/15, Loss: 0.4246, Train Acc: 84.35%, Precision: 0.8467, Recall: 0.8435, F1: 0.8438\n",
      "Epoch 4/15, Loss: 0.3560, Train Acc: 87.01%, Precision: 0.8734, Recall: 0.8701, F1: 0.8703\n",
      "Epoch 5/15, Loss: 0.3425, Train Acc: 87.01%, Precision: 0.8730, Recall: 0.8701, F1: 0.8705\n",
      "Epoch 6/15, Loss: 0.2964, Train Acc: 88.98%, Precision: 0.8929, Recall: 0.8898, F1: 0.8902\n",
      "Epoch 7/15, Loss: 0.2696, Train Acc: 89.85%, Precision: 0.9011, Recall: 0.8985, F1: 0.8987\n",
      "Epoch 8/15, Loss: 0.2383, Train Acc: 90.89%, Precision: 0.9111, Recall: 0.9089, F1: 0.9091\n",
      "Epoch 9/15, Loss: 0.2323, Train Acc: 91.14%, Precision: 0.9137, Recall: 0.9114, F1: 0.9117\n",
      "Epoch 10/15, Loss: 0.2186, Train Acc: 91.83%, Precision: 0.9198, Recall: 0.9183, F1: 0.9185\n",
      "Epoch 11/15, Loss: 0.2039, Train Acc: 92.21%, Precision: 0.9236, Recall: 0.9221, F1: 0.9222\n",
      "Epoch 12/15, Loss: 0.1861, Train Acc: 92.99%, Precision: 0.9311, Recall: 0.9299, F1: 0.9300\n",
      "Epoch 13/15, Loss: 0.1775, Train Acc: 93.41%, Precision: 0.9348, Recall: 0.9341, F1: 0.9342\n",
      "Epoch 14/15, Loss: 0.1744, Train Acc: 93.45%, Precision: 0.9355, Recall: 0.9345, F1: 0.9346\n",
      "Epoch 15/15, Loss: 0.1871, Train Acc: 93.33%, Precision: 0.9342, Recall: 0.9333, F1: 0.9334\n",
      "Test Accuracy: 93.58%, Precision: 0.9374, Recall: 0.9358, F1: 0.9357\n",
      "Confusion Matrix:\n",
      " [[546   4   6]\n",
      " [  4 531  59]\n",
      " [ 33   9 600]]\n"
     ]
    }
   ],
   "source": [
    "model_1 = CNNModel(num_classes=3)\n",
    "trained_model_1 = train_and_evaluate_more_metrics(model_1, train_loader, test_loader, epochs=15)\n",
    "torch.save(trained_model_1.state_dict(), \"models_output/model1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 65 razy pomylił inne klasy jako Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodanie innych metryk nie wniosło nowych informacji poza tym, że dane są dobrze zbalansowane oraz model dobrze generalizuje.\n",
    "Interpretacja ConfusionMatrix: Na przekątnej wartości dobrze zidentyfikowane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model zmienił funkcję aktywacji z ReLU na LeakyReLU, która przepuszcza wartośći ujemne w pewnym stopniu (ustawiony na 0.1).\n",
    "ReLU : jeśli otrzyma dużą liczbę ujmenych wartości to gradient może zatrzymać się na 0 i przestać się uczyć.\n",
    "LeakyReLU: jesli otrzyma dużą liczbę ujmenych wartość to gradient nie będzie 0 tylko mały i proces uczenia nie zatrzyma się."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel_gradient(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(CNNModel_gradient, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.1),  \\\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.1),  \n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.1),  \n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 28 * 28, 512),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5), # zapobieganie overfittingowi\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/15, Loss: 0.9003, Train Acc: 59.24%, Precision: 0.6072, Recall: 0.5924, F1: 0.5945\n",
      "Epoch 2/15, Loss: 0.6314, Train Acc: 74.73%, Precision: 0.7520, Recall: 0.7473, F1: 0.7482\n",
      "Epoch 3/15, Loss: 0.4641, Train Acc: 82.83%, Precision: 0.8304, Recall: 0.8283, F1: 0.8285\n",
      "Epoch 4/15, Loss: 0.3848, Train Acc: 85.47%, Precision: 0.8569, Recall: 0.8547, F1: 0.8549\n",
      "Epoch 5/15, Loss: 0.3880, Train Acc: 85.35%, Precision: 0.8561, Recall: 0.8535, F1: 0.8537\n",
      "Epoch 6/15, Loss: 0.3433, Train Acc: 87.32%, Precision: 0.8760, Recall: 0.8732, F1: 0.8735\n",
      "Epoch 7/15, Loss: 0.3057, Train Acc: 88.48%, Precision: 0.8866, Recall: 0.8848, F1: 0.8850\n",
      "Epoch 8/15, Loss: 0.3159, Train Acc: 88.14%, Precision: 0.8839, Recall: 0.8814, F1: 0.8816\n",
      "Epoch 9/15, Loss: 0.3192, Train Acc: 88.27%, Precision: 0.8849, Recall: 0.8827, F1: 0.8829\n",
      "Epoch 10/15, Loss: 0.2536, Train Acc: 90.41%, Precision: 0.9058, Recall: 0.9041, F1: 0.9043\n",
      "Epoch 11/15, Loss: 0.2781, Train Acc: 89.60%, Precision: 0.8978, Recall: 0.8960, F1: 0.8962\n",
      "Epoch 12/15, Loss: 0.2465, Train Acc: 91.11%, Precision: 0.9125, Recall: 0.9111, F1: 0.9113\n",
      "Epoch 13/15, Loss: 0.2613, Train Acc: 90.43%, Precision: 0.9061, Recall: 0.9043, F1: 0.9045\n",
      "Epoch 14/15, Loss: 0.2250, Train Acc: 91.61%, Precision: 0.9175, Recall: 0.9161, F1: 0.9162\n",
      "Epoch 15/15, Loss: 0.2353, Train Acc: 91.55%, Precision: 0.9169, Recall: 0.9155, F1: 0.9157\n",
      "Test Accuracy: 92.97%, Precision: 0.9351, Recall: 0.9297, F1: 0.9299\n",
      "Confusion Matrix:\n",
      " [[533   0  23]\n",
      " [  7 512  75]\n",
      " [ 16   5 621]]\n"
     ]
    }
   ],
   "source": [
    "model2 = CNNModel_gradient(num_classes=3)\n",
    "trained_model2 = train_and_evaluate_more_metrics(model2, train_loader, test_loader, epochs=15)\n",
    "torch.save(trained_model2.state_dict(), \"models_output/model2.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeakyReLU może powodować, że gradienty są mniejsze dla wartości ujemnych, co może spowolnić uczenie się w pewnych przypadkach. -> widzimy spadek accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmiana nn.Dropout na 0.2 -> sprawdzenie czy w przypadku 1 modelu nie zachodizło zjawisko underfittingu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel_lower_dropout(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(CNNModel_lower_dropout, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 28 * 28, 512),  # ewentualnie dodać jeszcze jedną warstwę pośrednią\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2), # zapobieganie overfittingowi\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/15, Loss: 0.9589, Train Acc: 60.28%, Precision: 0.6126, Recall: 0.6028, F1: 0.6042\n",
      "Epoch 2/15, Loss: 0.6386, Train Acc: 73.76%, Precision: 0.7424, Recall: 0.7376, F1: 0.7389\n",
      "Epoch 3/15, Loss: 0.5358, Train Acc: 79.52%, Precision: 0.7975, Recall: 0.7952, F1: 0.7955\n",
      "Epoch 4/15, Loss: 0.4485, Train Acc: 83.06%, Precision: 0.8331, Recall: 0.8306, F1: 0.8309\n",
      "Epoch 5/15, Loss: 0.4163, Train Acc: 83.85%, Precision: 0.8412, Recall: 0.8385, F1: 0.8389\n",
      "Epoch 6/15, Loss: 0.3564, Train Acc: 86.61%, Precision: 0.8679, Recall: 0.8661, F1: 0.8663\n",
      "Epoch 7/15, Loss: 0.3125, Train Acc: 88.27%, Precision: 0.8844, Recall: 0.8827, F1: 0.8829\n",
      "Epoch 8/15, Loss: 0.2734, Train Acc: 89.67%, Precision: 0.8989, Recall: 0.8967, F1: 0.8969\n",
      "Epoch 9/15, Loss: 0.2622, Train Acc: 90.00%, Precision: 0.9011, Recall: 0.9000, F1: 0.9001\n",
      "Epoch 10/15, Loss: 0.2211, Train Acc: 91.33%, Precision: 0.9145, Recall: 0.9133, F1: 0.9135\n",
      "Epoch 11/15, Loss: 0.2190, Train Acc: 91.77%, Precision: 0.9190, Recall: 0.9177, F1: 0.9179\n",
      "Epoch 12/15, Loss: 0.1889, Train Acc: 93.09%, Precision: 0.9316, Recall: 0.9309, F1: 0.9310\n",
      "Epoch 13/15, Loss: 0.1857, Train Acc: 93.30%, Precision: 0.9335, Recall: 0.9330, F1: 0.9331\n",
      "Epoch 14/15, Loss: 0.1669, Train Acc: 93.87%, Precision: 0.9390, Recall: 0.9387, F1: 0.9388\n",
      "Epoch 15/15, Loss: 0.1639, Train Acc: 94.17%, Precision: 0.9421, Recall: 0.9417, F1: 0.9418\n",
      "Test Accuracy: 93.97%, Precision: 0.9414, Recall: 0.9397, F1: 0.9401\n",
      "Confusion Matrix:\n",
      " [[515   0  41]\n",
      " [  3 560  31]\n",
      " [ 11  22 609]]\n"
     ]
    }
   ],
   "source": [
    "model3 = CNNModel_lower_dropout(num_classes=3)\n",
    "trained_model3 = train_and_evaluate_more_metrics(model3, train_loader, test_loader, epochs=15)\n",
    "torch.save(trained_model3.state_dict(), \"models_output/model3.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwiększenie ilości filtrów. dla małego zbiru danych może nie być najlepsze, ponieważ model może zacząć się uczyć na pamięć."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel_more_filters(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(CNNModel_more_filters, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 28 * 28, 512),  \n",
    "            # -> tu ustawiamy liczbę filtrów w ostatniej warstwie konwolucyjnej \n",
    "            # czyli Conv2d razy wymiary po wszystkich maxPoolach\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5), # zapobieganie overfittingowi\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/15, Loss: 1.0083, Train Acc: 59.67%, Precision: 0.6128, Recall: 0.5967, F1: 0.5972\n",
      "Epoch 2/15, Loss: 0.7237, Train Acc: 69.63%, Precision: 0.7038, Recall: 0.6963, F1: 0.6979\n",
      "Epoch 3/15, Loss: 0.5551, Train Acc: 78.18%, Precision: 0.7844, Recall: 0.7818, F1: 0.7821\n",
      "Epoch 4/15, Loss: 0.4604, Train Acc: 82.69%, Precision: 0.8286, Recall: 0.8269, F1: 0.8270\n",
      "Epoch 5/15, Loss: 0.3988, Train Acc: 85.36%, Precision: 0.8557, Recall: 0.8536, F1: 0.8537\n",
      "Epoch 6/15, Loss: 0.3462, Train Acc: 87.02%, Precision: 0.8725, Recall: 0.8702, F1: 0.8705\n",
      "Epoch 7/15, Loss: 0.3159, Train Acc: 88.30%, Precision: 0.8855, Recall: 0.8830, F1: 0.8833\n",
      "Epoch 8/15, Loss: 0.2775, Train Acc: 89.75%, Precision: 0.8999, Recall: 0.8975, F1: 0.8978\n",
      "Epoch 9/15, Loss: 0.2421, Train Acc: 90.74%, Precision: 0.9092, Recall: 0.9074, F1: 0.9075\n",
      "Epoch 10/15, Loss: 0.2564, Train Acc: 90.56%, Precision: 0.9078, Recall: 0.9056, F1: 0.9058\n",
      "Epoch 11/15, Loss: 0.2535, Train Acc: 90.67%, Precision: 0.9081, Recall: 0.9067, F1: 0.9068\n",
      "Epoch 12/15, Loss: 0.2081, Train Acc: 92.37%, Precision: 0.9248, Recall: 0.9237, F1: 0.9238\n",
      "Epoch 13/15, Loss: 0.1926, Train Acc: 92.96%, Precision: 0.9305, Recall: 0.9296, F1: 0.9297\n",
      "Epoch 14/15, Loss: 0.2027, Train Acc: 92.37%, Precision: 0.9246, Recall: 0.9237, F1: 0.9238\n",
      "Epoch 15/15, Loss: 0.1840, Train Acc: 93.52%, Precision: 0.9359, Recall: 0.9352, F1: 0.9353\n",
      "Test Accuracy: 92.80%, Precision: 0.9307, Recall: 0.9280, F1: 0.9278\n",
      "Confusion Matrix:\n",
      " [[548   2   6]\n",
      " [  8 520  66]\n",
      " [ 40   7 595]]\n"
     ]
    }
   ],
   "source": [
    "model4 = CNNModel_more_filters(num_classes=3)\n",
    "trained_model4 = train_and_evaluate_more_metrics(model4, train_loader, test_loader, epochs=15)\n",
    "torch.save(trained_model4.state_dict(), \"models_output/model4.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwiększenie warstw o jeden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel_more_layers(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(CNNModel_more_layers, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)  # Nowa warstwa MaxPooling\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 14 * 14, 512),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5), # zapobieganie overfittingowi\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/15, Loss: 0.9154, Train Acc: 56.61%, Precision: 0.5772, Recall: 0.5661, F1: 0.5624\n",
      "Epoch 2/15, Loss: 0.6970, Train Acc: 70.60%, Precision: 0.7139, Recall: 0.7060, F1: 0.7081\n",
      "Epoch 3/15, Loss: 0.5687, Train Acc: 77.65%, Precision: 0.7800, Recall: 0.7765, F1: 0.7771\n",
      "Epoch 4/15, Loss: 0.3980, Train Acc: 85.22%, Precision: 0.8541, Recall: 0.8522, F1: 0.8524\n",
      "Epoch 5/15, Loss: 0.3187, Train Acc: 88.06%, Precision: 0.8831, Recall: 0.8806, F1: 0.8809\n",
      "Epoch 6/15, Loss: 0.2889, Train Acc: 89.24%, Precision: 0.8947, Recall: 0.8924, F1: 0.8927\n",
      "Epoch 7/15, Loss: 0.2389, Train Acc: 90.77%, Precision: 0.9095, Recall: 0.9077, F1: 0.9079\n",
      "Epoch 8/15, Loss: 0.2262, Train Acc: 91.63%, Precision: 0.9176, Recall: 0.9163, F1: 0.9164\n",
      "Epoch 9/15, Loss: 0.2038, Train Acc: 92.53%, Precision: 0.9264, Recall: 0.9253, F1: 0.9254\n",
      "Epoch 10/15, Loss: 0.1894, Train Acc: 92.46%, Precision: 0.9253, Recall: 0.9246, F1: 0.9247\n",
      "Epoch 11/15, Loss: 0.1957, Train Acc: 92.69%, Precision: 0.9279, Recall: 0.9269, F1: 0.9270\n",
      "Epoch 12/15, Loss: 0.1705, Train Acc: 93.43%, Precision: 0.9354, Recall: 0.9343, F1: 0.9345\n",
      "Epoch 13/15, Loss: 0.1647, Train Acc: 93.98%, Precision: 0.9404, Recall: 0.9398, F1: 0.9399\n",
      "Epoch 14/15, Loss: 0.1543, Train Acc: 94.19%, Precision: 0.9424, Recall: 0.9419, F1: 0.9420\n",
      "Epoch 15/15, Loss: 0.1347, Train Acc: 95.19%, Precision: 0.9522, Recall: 0.9519, F1: 0.9520\n",
      "Test Accuracy: 94.75%, Precision: 0.9479, Recall: 0.9475, F1: 0.9475\n",
      "Confusion Matrix:\n",
      " [[549   1   6]\n",
      " [  0 551  43]\n",
      " [ 29  15 598]]\n"
     ]
    }
   ],
   "source": [
    "model5 = CNNModel_more_layers(num_classes=3)\n",
    "trained_model5 = train_and_evaluate_more_metrics(model5, train_loader, test_loader, epochs=15)\n",
    "torch.save(trained_model5.state_dict(), \"models_output/model5.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6\n",
    "W stosunku do modelu 5 zmniejszono dropout do 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel_more_layers_lower_dropout(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(CNNModel_more_layers_lower_dropout, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2) \n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 14 * 14, 512), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3), \n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/15, Loss: 0.8967, Train Acc: 58.85%, Precision: 0.5997, Recall: 0.5885, F1: 0.5871\n",
      "Epoch 2/15, Loss: 0.6860, Train Acc: 71.34%, Precision: 0.7197, Recall: 0.7134, F1: 0.7149\n",
      "Epoch 3/15, Loss: 0.4715, Train Acc: 82.45%, Precision: 0.8263, Recall: 0.8245, F1: 0.8246\n",
      "Epoch 4/15, Loss: 0.3922, Train Acc: 85.34%, Precision: 0.8556, Recall: 0.8534, F1: 0.8535\n",
      "Epoch 5/15, Loss: 0.3171, Train Acc: 88.54%, Precision: 0.8875, Recall: 0.8854, F1: 0.8855\n",
      "Epoch 6/15, Loss: 0.2886, Train Acc: 89.04%, Precision: 0.8927, Recall: 0.8904, F1: 0.8906\n",
      "Epoch 7/15, Loss: 0.2808, Train Acc: 89.60%, Precision: 0.8979, Recall: 0.8960, F1: 0.8961\n",
      "Epoch 8/15, Loss: 0.2331, Train Acc: 91.19%, Precision: 0.9134, Recall: 0.9119, F1: 0.9121\n",
      "Epoch 9/15, Loss: 0.2175, Train Acc: 91.95%, Precision: 0.9208, Recall: 0.9195, F1: 0.9196\n",
      "Epoch 10/15, Loss: 0.2000, Train Acc: 92.56%, Precision: 0.9268, Recall: 0.9256, F1: 0.9257\n",
      "Epoch 11/15, Loss: 0.1883, Train Acc: 93.07%, Precision: 0.9317, Recall: 0.9307, F1: 0.9308\n",
      "Epoch 12/15, Loss: 0.1729, Train Acc: 93.91%, Precision: 0.9400, Recall: 0.9391, F1: 0.9392\n",
      "Epoch 13/15, Loss: 0.1744, Train Acc: 93.43%, Precision: 0.9350, Recall: 0.9343, F1: 0.9344\n",
      "Epoch 14/15, Loss: 0.1598, Train Acc: 94.11%, Precision: 0.9417, Recall: 0.9411, F1: 0.9412\n",
      "Epoch 15/15, Loss: 0.1595, Train Acc: 94.04%, Precision: 0.9411, Recall: 0.9404, F1: 0.9405\n",
      "Test Accuracy: 95.59%, Precision: 0.9561, Recall: 0.9559, F1: 0.9558\n",
      "Confusion Matrix:\n",
      " [[550   3   3]\n",
      " [  3 568  23]\n",
      " [ 27  20 595]]\n"
     ]
    }
   ],
   "source": [
    "model6 = CNNModel_more_layers_lower_dropout(num_classes=3)\n",
    "trained_model6 = train_and_evaluate_more_metrics(model6, train_loader, test_loader, epochs=15)\n",
    "torch.save(trained_model6.state_dict(), \"models_output/model6.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najlepsze acc = 95.56% -> model 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inne podejście do balansowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ulcer': 0, 'AVM': 1, 'Normal': 2}\n"
     ]
    }
   ],
   "source": [
    "X, y = save_to_list(\"output\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123, stratify=y)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=123, stratify=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wczytanie modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel(num_classes=3)  \n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
