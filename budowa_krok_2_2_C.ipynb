{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ścieżka do folderu z oryginalnymi obrazami\n",
    "\n",
    "def save_to_list():\n",
    "    output_root = \"output\"\n",
    "    X = []\n",
    "    y = []\n",
    "    label_to_index = {label: idx for idx, label in enumerate(set(['AVM', 'Normal', 'Ulcer']))} \n",
    "    # Przechodzimy rekurencyjnie przez wszystkie pliki w folderze archive\n",
    "    for root, _, files in os.walk(output_root):\n",
    "        for file in files:\n",
    "            if file.endswith(\".bmp\"):  # Obsługujemy tylko pliki BMP\n",
    "                input_path = os.path.join(root, file)\n",
    "\n",
    "                base, ext = os.path.splitext(file)\n",
    "                new_filename = f\"{base}{ext}\"\n",
    "                image = cv2.imread(input_path)\n",
    "                # gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                #gray zostało tymczasowo zakomentowane aby sprawdzić accuracy\n",
    "                \n",
    "                # zapisanie obrazu w grayscale (3x mniej danych)\n",
    "                # TODO przezkalowanie z [0,255] do [0,1]\n",
    "                X.append(image)\n",
    "                y.append(label_to_index[new_filename.split(\"_\")[0]])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# data_all = save_to_list()\n",
    "# print(np.shape(data_all))\n",
    "X, y = save_to_list()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123, stratify=y)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=123, stratify=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dalsze przygotowanie danych (przetwarzanie i augmentacja)\n",
    "Ponieważ ostatni kamień milowy zakończyliśmy z ilością około 3000 zdjęć o rozmiarach 512 x 512 px powinniśmy postarać się zwiększyć ilość zdjęć oraz zmiejszyć ich rozmiar (i dosłownie do 224 x 224 px (standard w DL), i w pamięci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703/2\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform # przygotowanie do zwiększenia ilości zdjęć\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.fromarray(self.images[idx])  \n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiowanie transforamtorów i przypisanie do datasetów PyTorcha\n",
    "Ponieważ do tej pory używaliśmy sklearn a potrzebujemy pyTorcha (jest lepszy do wybranego przez nas algorytmu uczenia) to musimy przekonwertować dane na format który PyTorch będzie rozumiał"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([#TODO to jest placeholder \n",
    "    transforms.Resize((224, 224)),          # rozmiar do potencjalnej zmiany\n",
    "    transforms.RandomRotation((-180,180)),  \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),                  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([])  # do testów - bez zmian\n",
    "\n",
    "\n",
    "train_dataset = ImageDataset(X_train, y_train, transform=train_transform)\n",
    "test_dataset = ImageDataset(X_test, y_test, transform=test_transform)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[-2.0837, -2.0837, -2.0837,  ..., -2.0837, -2.0837, -2.0837],\n",
      "          [-2.0837, -2.0837, -2.0837,  ..., -2.0837, -2.0837, -2.0837],\n",
      "          [-2.0837, -2.0837, -2.0837,  ..., -2.0837, -2.0837, -2.0837],\n",
      "          ...,\n",
      "          [-2.0837, -2.0837, -2.0837,  ..., -2.0837, -2.0837, -2.0837],\n",
      "          [-2.0837, -2.0837, -2.0837,  ..., -2.0837, -2.0837, -2.0837],\n",
      "          [-2.0837, -2.0837, -2.0837,  ..., -2.0837, -2.0837, -2.0837]],\n",
      "\n",
      "         [[-2.0007, -2.0007, -2.0007,  ..., -2.0007, -2.0007, -2.0007],\n",
      "          [-2.0007, -2.0007, -2.0007,  ..., -2.0007, -2.0007, -2.0007],\n",
      "          [-2.0007, -2.0007, -2.0007,  ..., -2.0007, -2.0007, -2.0007],\n",
      "          ...,\n",
      "          [-2.0007, -2.0007, -2.0007,  ..., -2.0007, -2.0007, -2.0007],\n",
      "          [-2.0007, -2.0007, -2.0007,  ..., -2.0007, -2.0007, -2.0007],\n",
      "          [-2.0007, -2.0007, -2.0007,  ..., -2.0007, -2.0007, -2.0007]],\n",
      "\n",
      "         [[-1.7696, -1.7696, -1.7696,  ..., -1.7696, -1.7696, -1.7696],\n",
      "          [-1.7696, -1.7696, -1.7696,  ..., -1.7696, -1.7696, -1.7696],\n",
      "          [-1.7696, -1.7696, -1.7696,  ..., -1.7696, -1.7696, -1.7696],\n",
      "          ...,\n",
      "          [-1.7696, -1.7696, -1.7696,  ..., -1.7696, -1.7696, -1.7696],\n",
      "          [-1.7696, -1.7696, -1.7696,  ..., -1.7696, -1.7696, -1.7696],\n",
      "          [-1.7696, -1.7696, -1.7696,  ..., -1.7696, -1.7696, -1.7696]]],\n",
      "\n",
      "\n",
      "        [[[-1.9980, -1.9980, -1.9980,  ..., -1.9980, -1.9980, -1.9980],\n",
      "          [-1.9980, -1.9980, -1.9980,  ..., -1.9980, -1.9980, -1.9980],\n",
      "          [-1.9980, -1.9980, -1.9980,  ..., -1.9980, -1.9980, -1.9980],\n",
      "          ...,\n",
      "          [-1.9980, -1.9980, -1.9980,  ..., -1.9980, -1.9980, -1.9980],\n",
      "          [-1.9980, -1.9980, -1.9980,  ..., -1.9980, -1.9980, -1.9980],\n",
      "          [-1.9980, -1.9980, -1.9980,  ..., -1.9980, -1.9980, -1.9980]],\n",
      "\n",
      "         [[-1.9132, -1.9132, -1.9132,  ..., -1.9132, -1.9132, -1.9132],\n",
      "          [-1.9132, -1.9132, -1.9132,  ..., -1.9132, -1.9132, -1.9132],\n",
      "          [-1.9132, -1.9132, -1.9132,  ..., -1.9132, -1.9132, -1.9132],\n",
      "          ...,\n",
      "          [-1.9132, -1.9132, -1.9132,  ..., -1.9132, -1.9132, -1.9132],\n",
      "          [-1.9132, -1.9132, -1.9132,  ..., -1.9132, -1.9132, -1.9132],\n",
      "          [-1.9132, -1.9132, -1.9132,  ..., -1.9132, -1.9132, -1.9132]],\n",
      "\n",
      "         [[-1.6824, -1.6824, -1.6824,  ..., -1.6824, -1.6824, -1.6824],\n",
      "          [-1.6824, -1.6824, -1.6824,  ..., -1.6824, -1.6824, -1.6824],\n",
      "          [-1.6824, -1.6824, -1.6824,  ..., -1.6824, -1.6824, -1.6824],\n",
      "          ...,\n",
      "          [-1.6824, -1.6824, -1.6824,  ..., -1.6824, -1.6824, -1.6824],\n",
      "          [-1.6824, -1.6824, -1.6824,  ..., -1.6824, -1.6824, -1.6824],\n",
      "          [-1.6824, -1.6824, -1.6824,  ..., -1.6824, -1.6824, -1.6824]]]])\n",
      "tensor([1, 1, 1, 2, 0, 1, 1, 0, 1, 1, 1, 0, 1, 2, 1, 0, 1, 1, 1, 2, 0, 2, 0, 2,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wybór modelu\n",
    "Wstępnie wybraliśmy algorytm CNN (Convolutional Neural Networks) ponieważ:\n",
    "- Ilość zdjęć po transformacjach będzie wynosiła dziesiątki tysięcy co sprawia, że użycie tego algorytmu możliwe (ale idealnie byłoby aby miec diesiątki tysięcy oryginalnych zdjęć)\n",
    "- Model powinien umieć rozpoznawać fragmenty zdjęcia do czego nadają się CNNy (robią to automatycznie)\n",
    "- Jest najlepszym algorytmem do klasyfikacji obrazów (w naszej opinii)\n",
    "\n",
    "jeśli starczy czasu spróbujemy stworzyć też inne modele aby porównać celność rożnych algorytmów.\n",
    "- Transfer Learning - może być bardzo użyteczny ponieważ bez transformacji mamy 3000 tylko zdjęć (może byc overfitting przy CNN)\n",
    "#### Powody przeciwko wybraniu pozotałych algorytmów:\n",
    "- SVM - zbyt wolny przy dużej ilości danych\n",
    "- Gradient Boosting - Nadaje się do danych tabelarycznych  - nie obrazów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 28 * 28, 512),  # ewentualnie dodać jeszcze jedną warstwę pośrednią\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5), # zapobieganie overfittingowi do wyregulowania\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wyjaśnienie kodu\n",
    "\n",
    "self.conv_layers - zamienia tensor 224×224×3 na tensor 28×28×128, który zawiera ekstraktowane cechy obrazu.\n",
    "\n",
    "fc_layers - warstwy klasyfikacyjne (ta część która jest okładką deepLearningu) (input - nodes - output)\n",
    "(ewentualna możliwość dodania dodatkowej warstwy nodes w razie wymagań)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
