{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ścieżka do folderu z oryginalnymi obrazami\n",
    "\n",
    "def save_to_list():\n",
    "    output_root = \"output\"\n",
    "    X = []\n",
    "    y = []\n",
    "    label_to_index = {label: idx for idx, label in enumerate(set(['AVM', 'Normal', 'Ulcer']))} \n",
    "    # Przechodzimy rekurencyjnie przez wszystkie pliki w folderze archive\n",
    "    for root, _, files in os.walk(output_root):\n",
    "        for file in files:\n",
    "            if file.endswith(\".bmp\"):  # Obsługujemy tylko pliki BMP\n",
    "                input_path = os.path.join(root, file)\n",
    "\n",
    "                base, ext = os.path.splitext(file)\n",
    "                new_filename = f\"{base}{ext}\"\n",
    "                image = cv2.imread(input_path)\n",
    "                # gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                #gray zostało tymczasowo zakomentowane aby sprawdzić accuracy\n",
    "                \n",
    "                # zapisanie obrazu w grayscale (3x mniej danych)\n",
    "                # TODO przezkalowanie z [0,255] do [0,1]\n",
    "                X.append(image)\n",
    "                y.append(label_to_index[new_filename.split(\"_\")[0]])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# data_all = save_to_list()\n",
    "# print(np.shape(data_all))\n",
    "X, y = save_to_list()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123, stratify=y)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=123, stratify=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dalsze przygotowanie danych (przetwarzanie i augmentacja)\n",
    "Ponieważ ostatni kamień milowy zakończyliśmy z ilością około 3000 zdjęć o rozmiarach 512 x 512 px powinniśmy postarać się zwiększyć ilość zdjęć oraz zmiejszyć ich rozmiar (i dosłownie do 224 x 224 px (standard w DL), i w pamięci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703/2\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform # przygotowanie do zwiększenia ilości zdjęć\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.fromarray(self.images[idx])  \n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiowanie transforamtorów i przypisanie do datasetów PyTorcha\n",
    "Ponieważ do tej pory używaliśmy sklearn a potrzebujemy pyTorcha (jest lepszy do wybranego przez nas algorytmu uczenia) to musimy przekonwertować dane na format który PyTorch będzie rozumiał"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([#TODO to jest placeholder \n",
    "    transforms.Resize((224, 224)),          # rozmiar do potencjalnej zmiany\n",
    "    transforms.RandomRotation((-180,180)),  \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),                  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])  # do testów - bez zmian\n",
    "\n",
    "\n",
    "train_dataset = ImageDataset(X_train, y_train, transform=train_transform)\n",
    "test_dataset = ImageDataset(X_test, y_test, transform=test_transform)\n",
    "\n",
    "class_counts = np.bincount(y)  \n",
    "class_weights = 1.0 / class_counts \n",
    "\n",
    "sample_weights_tr = [class_weights[label] for label in y_train]\n",
    "sample_weights_ts = [class_weights[label] for label in y_test]\n",
    "\n",
    "samplertr = WeightedRandomSampler(sample_weights_tr, num_samples= len(sample_weights_tr), replacement=True)\n",
    "samplerts = WeightedRandomSampler(sample_weights_ts, num_samples= len(sample_weights_ts), replacement=True)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler = samplertr)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler = samplerts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 2, 1, 1, 0, 2, 0, 0, 2, 2, 2, 1, 2, 0, 2, 2, 1, 2, 0, 0, 0, 2, 0,\n",
      "        0, 2, 1, 2, 0, 2, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wybór modelu\n",
    "Wstępnie wybraliśmy algorytm CNN (Convolutional Neural Networks) ponieważ:\n",
    "- Ilość zdjęć po transformacjach będzie wynosiła dziesiątki tysięcy co sprawia, że użycie tego algorytmu możliwe (ale idealnie byłoby aby miec diesiątki tysięcy oryginalnych zdjęć)\n",
    "- Model powinien umieć rozpoznawać fragmenty zdjęcia do czego nadają się CNNy (robią to automatycznie)\n",
    "- Jest najlepszym algorytmem do klasyfikacji obrazów (w naszej opinii)\n",
    "\n",
    "jeśli starczy czasu spróbujemy stworzyć też inne modele aby porównać celność rożnych algorytmów.\n",
    "- Transfer Learning - może być bardzo użyteczny ponieważ bez transformacji mamy 3000 tylko zdjęć (może byc overfitting przy CNN)\n",
    "#### Powody przeciwko wybraniu pozotałych algorytmów:\n",
    "- SVM - zbyt wolny przy dużej ilości danych\n",
    "- Gradient Boosting - Nadaje się do danych tabelarycznych  - nie obrazów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 28 * 28, 512),  # ewentualnie dodać jeszcze jedną warstwę pośrednią\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3), # zapobieganie overfittingowi do wyregulowania\n",
    "            nn.Linear(512, 512),  # ewentualnie dodać jeszcze jedną warstwę pośrednią\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3), # zapobieganie overfittingowi do wyregulowania\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wyjaśnienie kodu\n",
    "\n",
    "self.conv_layers - zamienia tensor 224×224×3 na tensor 28×28×128, który zawiera ekstraktowane cechy obrazu.\n",
    "\n",
    "fc_layers - warstwy klasyfikacyjne (ta część która jest okładką deepLearningu) (input - nodes - output)\n",
    "(ewentualna możliwość dodania dodatkowej warstwy nodes w razie wymagań)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, test_loader, epochs=10, learning_rate=0.001, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_acc = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_acc:.2f}%\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 3.0826, Train Accuracy: 45.47%\n",
      "Epoch 2/15, Loss: 0.9284, Train Accuracy: 54.53%\n",
      "Epoch 3/15, Loss: 0.9020, Train Accuracy: 59.92%\n",
      "Epoch 4/15, Loss: 0.8529, Train Accuracy: 62.14%\n",
      "Epoch 5/15, Loss: 0.8432, Train Accuracy: 60.01%\n",
      "Epoch 6/15, Loss: 0.8259, Train Accuracy: 62.40%\n",
      "Epoch 7/15, Loss: 0.8172, Train Accuracy: 62.84%\n",
      "Epoch 8/15, Loss: 0.8143, Train Accuracy: 64.27%\n",
      "Epoch 9/15, Loss: 0.7789, Train Accuracy: 65.45%\n",
      "Epoch 10/15, Loss: 0.7570, Train Accuracy: 67.49%\n",
      "Epoch 11/15, Loss: 0.7727, Train Accuracy: 66.32%\n",
      "Epoch 12/15, Loss: 0.7768, Train Accuracy: 66.19%\n",
      "Epoch 13/15, Loss: 0.7588, Train Accuracy: 66.62%\n",
      "Epoch 14/15, Loss: 0.7109, Train Accuracy: 69.97%\n",
      "Epoch 15/15, Loss: 0.7302, Train Accuracy: 69.10%\n",
      "Test Accuracy: 75.86%\n"
     ]
    }
   ],
   "source": [
    "model = CNNModel(num_classes=3)\n",
    "trained_model = train_and_evaluate(model, train_loader, test_loader, epochs=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
